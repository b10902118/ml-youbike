{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from pathlib import Path\n","import pandas as pd\n","import ujson as json\n","from tqdm import tqdm\n","from concurrent.futures import ProcessPoolExecutor\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","import numpy as np\n","from matplotlib import pyplot as plt\n","from tqdm import trange\n","from sklearn.cluster import KMeans\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import make_pipeline"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["DATA_DIR = Path(\"../html.2023.final.data\")\n","DEMOGRAPHICS_PATH = DATA_DIR / \"demographic.json\"\n","RELEASE_DIR = DATA_DIR / \"release\"\n","SNO_TEST_SET = DATA_DIR / \"sno_test_set.txt\"\n","SMALL_CACHE_DIR = Path(\"./cache\")"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def load_demographics(path: Path):\n","    with open(path) as f:\n","        demo = json.load(f)  # { key: DATA }\n","        ar = []\n","        for k, v in demo.items():\n","            v[\"sno\"] = k  # station number\n","            ar.append(v)\n","    return pd.DataFrame(ar)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def load_data_file(path: Path, base_date=\"20231001\"):\n","    with open(path) as f:\n","        data = json.load(f)\n","        ar = []\n","        for k, v in data.items():\n","            t = pd.to_datetime(f\"{base_date} {k}\")\n","            v[\"time\"] = t\n","            v[\"sno\"] = path.stem\n","            ar.append(v)\n","    return pd.DataFrame(ar).bfill(limit=3)  # fill NaN with next value"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def load_all_data():\n","    df = pd.read_pickle(\"./cache/small_data_cache.pkl\")\n","    return df"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def get_station_sno_df(df, snos):\n","    ret = df[df[\"sno\"].isin(snos)].groupby(\"sno\")[\"tot\"].first().reset_index()\n","    ret.set_index(\"sno\", inplace=True)\n","    ret.columns = [\"tot\"]\n","    return ret"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["TRAIN_END = \"2023-10-30 23:59\"\n","TEST_START = \"2023-12-01 00:00\"\n","TEST_END = \"2023-12-07 23:59\"\n","ntu_snos = [l.strip() for l in open(SNO_TEST_SET).read().splitlines()]"]},{"cell_type":"markdown","metadata":{},"source":["print(\"Loading data...\")"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Amount of data points by stations count      112.000000\n","mean     96983.750000\n","std      11066.167991\n","min          0.000000\n","25%      98682.000000\n","50%      98682.000000\n","75%      98682.000000\n","max      98683.000000\n","Name: time, dtype: float64\n","Number of stations 111\n"]}],"source":["df = load_all_data()\n","df = df[df[\"act\"] == \"1\"]  # filter out inactive stations\n","station_sno_df = get_station_sno_df(df, ntu_snos)\n","print(\n","    \"Amount of data points by stations\", df.groupby(by=\"sno\")[\"time\"].size().describe()\n",")\n","print(\"Number of stations\", df[\"sno\"].nunique())\n","assert df.groupby(by=\"sno\")[\"time\"].is_monotonic_increasing.all(), \"WTF?!\""]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["bad_station = [\n","    \"500105087\",\n","    \"500108169\",\n","    \"500108170\",\n","]  # these two stations are added very late, so we remove them for simplicity\n","df = df[~df[\"sno\"].isin(bad_station)]"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["N_STATIONS = df[\"sno\"].nunique()"]},{"cell_type":"markdown","metadata":{},"source":["the data looks like this:"]},{"cell_type":"markdown","metadata":{},"source":["\n","<br>\n","                    time        sno   tot   sbi  bemp act<br>\n","0    2023-10-02 00:00:00  500101001  28.0  12.0  16.0   1<br>\n","1    2023-10-02 00:01:00  500101001  28.0  12.0  16.0   1<br>\n","2    2023-10-02 00:02:00  500101001  28.0  13.0  15.0   1<br>\n","...<br>\n"]},{"cell_type":"markdown","metadata":{},"source":["sno is the station number<br>\n","and we want to predict the value of sbi at time t+1"]},{"cell_type":"markdown","metadata":{},"source":["dataframe for stations' bike number at time t"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["dfp = df.pivot(index=\"time\", columns=\"sno\", values=\"sbi\")\n","never_active_snos = set(ntu_snos) - set(dfp.columns)\n","# preserve order\n","ntu_snos = [sno for sno in ntu_snos if sno not in never_active_snos]\n","# ntu_snos = list(set(ntu_snos) - never_active_snos)\n","dfp = dfp[ntu_snos]\n","time_split = pd.to_datetime(TRAIN_END)\n","train = dfp[dfp.index <= time_split].copy()\n","# train = dfp[(dfp.index >= pd.to_datetime(\"20231101 00:00:00\")) & (dfp.index <= time_split)]\n","test = dfp[(dfp.index >= TEST_START) & (dfp.index <= TEST_END)].copy()"]},{"cell_type":"markdown","metadata":{},"source":["use full data for training to get better performance on public test set"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["train = dfp.copy()"]},{"cell_type":"markdown","metadata":{},"source":["special holidays (only for training data as my method can't handle special holidays in weekday that is not in training data)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["long_holiday = (\n","    pd.date_range(start=\"2023-10-07\", end=\"2023-10-10\")\n","    # .union(pd.date_range(start=\"2023-11-15\", end=\"2023-11-15\"))\n","    # .union(pd.date_range(start=\"2023-11-24\", end=\"2023-11-24\"))\n",")"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def is_holiday(s):\n","    return s.dt.weekday.isin((5, 6))\n","    return s.isin(long_holiday) | s.dt.weekday.isin((5, 6))"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def quantile(x):\n","    return lambda y: y.quantile(x)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["base_index_names = [\n","    \"sno\",\n","    \"weekday\",\n","    \"hour\",\n","    \"minute\",\n","]\n","index_names = base_index_names + [\"is_holiday\"]\n","property_names = [\n","    \"mean_20\",\n","    \"std_20\",\n","    # \"var_20\",\n","    # \"skew_20\",\n","    # \"kurt_20\",\n","    \"q25_20\",\n","    \"q50_20\",\n","    \"q75_20\",\n","    \"mean_sta\",\n","    \"std_sta\",\n","    # \"var_sta\",\n","    # \"skew_sta\",\n","    # \"kurt_sta\",\n","    # \"mean_1h\",\n","    # \"std_1h\",\n","]"]},{"cell_type":"markdown","metadata":{},"source":["datetime_range = pd.date_range(\"2023/10/01 00:00\", \"2023/11/30 23:59\", freq=\"min\")<br>\n","datetime_df = pd.DataFrame(<br>\n","    {\"is_holiday\": is_holiday(datetime_range.to_series())},<br>\n","    index=datetime_range,<br>\n",")"]},{"cell_type":"markdown","metadata":{},"source":["to_group_df's index is `index_names` a.k.a. input<br>\n","to_group_df's columns are `property_names` a.k.a. properties"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["def prepare_data(train):\n","    # dataframe for time at each day\n","    day_time_20 = pd.date_range(\"2023/11/05 00:00\", \"2023/11/11 23:59\", freq=\"20min\")\n","    day_time_20_df = pd.DataFrame(\n","        {\n","            \"weekday\": day_time_20.weekday,\n","            \"hour\": day_time_20.hour,\n","            \"minute\": day_time_20.minute,\n","        }\n","    )\n","    to_group_df = station_sno_df.copy().reset_index()\n","    to_group_df = (\n","        to_group_df.merge(day_time_20_df, how=\"cross\")\n","        # .merge(pd.DataFrame({\"is_holiday\": [0, 1]}), how=\"cross\")\n","        # .merge(pd.DataFrame({\"is_daytime\": [0, 1]}), how=\"cross\")\n","    )\n","    to_group_df.set_index(base_index_names, inplace=True)\n","\n","    # Melt train DataFrame to long format\n","    long_train = (\n","        train.reset_index()\n","        .melt(id_vars=\"time\", var_name=\"sno\", value_name=\"sbi\")\n","        .dropna()\n","    )\n","\n","    # Extract hour and minute from time\n","    long_train[\"weekday\"] = long_train[\"time\"].dt.weekday\n","    long_train[\"hour\"] = long_train[\"time\"].dt.hour\n","    long_train[\"minute\"] = (\n","        long_train[\"time\"].dt.minute // 20\n","    ) * 20  # Grouping minutes into 20-min intervals\n","    long_train[\"is_holiday\"] = is_holiday(long_train[\"time\"])\n","    rg = long_train.set_index(\"time\").groupby(\"sno\").rolling(\"1D\")\n","    long_train[\"sbi_roll_1h\"] = rg[\"sbi\"].mean().values\n","\n","    # Group by sno, hour, and minute, then calculate mean and std\n","    aggregated_train = (\n","        long_train.groupby(index_names)\n","        .agg(\n","            mean_20=(\"sbi\", \"mean\"),\n","            std_20=(\"sbi\", \"std\"),\n","            var_20=(\"sbi\", \"var\"),\n","            skew_20=(\"sbi\", \"skew\"),\n","            kurt_20=(\"sbi\", lambda x: x.kurt()),\n","            q25_20=(\"sbi\", quantile(0.25)),\n","            q50_20=(\"sbi\", quantile(0.5)),\n","            q75_20=(\"sbi\", quantile(0.75)),\n","            mean_1h=(\"sbi_roll_1h\", \"mean\"),\n","            std_1h=(\"sbi_roll_1h\", \"std\"),\n","        )\n","        .fillna(0)\n","    )\n","    aggregated_train = (\n","        aggregated_train.reset_index()\n","        .merge(\n","            long_train.groupby(\"sno\").agg(\n","                mean_sta=(\"sbi\", \"mean\"),\n","                std_sta=(\"sbi\", \"std\"),\n","                var_sta=(\"sbi\", \"var\"),\n","                skew_sta=(\"sbi\", \"skew\"),\n","                kurt_sta=(\"sbi\", lambda x: x.kurt()),\n","            ),\n","            on=\"sno\",\n","        )\n","        .set_index(index_names)\n","    )\n","\n","    # Merge with to_group_df\n","    to_group_df = to_group_df.merge(\n","        aggregated_train, left_index=True, right_index=True, how=\"left\"\n","    )\n","    return long_train, aggregated_train, to_group_df"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["def error(y_true: np.ndarray, y_pred: np.ndarray, tots: np.ndarray) -> np.float64:\n","    return 3 * np.dot(\n","        np.abs((y_pred - y_true) / tots),\n","        np.abs(y_true / tots - 1 / 3) + np.abs(y_true / tots - 2 / 3),\n","    )"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["def brute(\n","    y_true: np.ndarray, tots: np.ndarray, step: np.float64 = 0.5\n",") -> (np.float64, np.float64):\n","    tots = tots[~np.isnan(y_true)]\n","    y_true = y_true[~np.isnan(y_true)]\n","    arr_len = y_true.shape[0]\n","    assert arr_len == tots.shape[0]\n","    best_sbi = 0.0\n","    best_err = 9999999999.0  # biggest\n","    for sbi in np.arange(0, max(tots), step):\n","        sbis = np.full(arr_len, sbi)\n","        err = error(y_true, sbis, tots)\n","        if err < best_err:\n","            best_sbi, best_err = sbi, err\n","    return best_sbi, best_err / arr_len"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["def first_greater_prefix_sum_idx(arr, target):\n","    prefix_sum = 0\n","    for i, element in enumerate(arr):\n","        prefix_sum += element\n","        if prefix_sum > target:\n","            return i"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["def optimal_median(y_true: np.ndarray, tot: int) -> (np.float64, np.float64):\n","    # assert len(y_true.shape) == 1, \"optimal_median: shape error\"\n","    # print(f\"y_true: {y_true}\")\n","    nan_indices = np.isnan(y_true)\n","\n","    # Use boolean indexing to drop NaN values\n","    y_true = y_true[~nan_indices]\n","    arr_len = y_true.shape[0]\n","    tots = np.full(arr_len, tot)\n","\n","    # generalized median\n","    y_sorted = np.sort(y_true)\n","    weight = np.abs(y_sorted / tot - 1 / 3) + np.abs(y_sorted / tot - 2 / 3)\n","    w_mid = np.sum(weight) / 2\n","    w_cur = 0\n","\n","    # if odd, first > w_mid\n","    # if even, before m1 must less than w_mid, m2 must greater than w_mid\n","    best_sbi = y_sorted[first_greater_prefix_sum_idx(weight, w_mid)]\n","    best_err = error(y_true, np.full(arr_len, best_sbi), tots)\n","    return best_sbi, best_err"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["def get_group_assignment_df(kmeans, time_range, aggregated_train):\n","    global tmp_df\n","    time_range = (\n","        time_range.to_series().resample(\"20min\").agg(\"first\").dropna().index.to_series()\n","    )\n","    tmp_df = pd.DataFrame(\n","        {\n","            \"time\": time_range,\n","            \"weekday\": time_range.dt.weekday,\n","            \"hour\": time_range.dt.hour,\n","            \"minute\": time_range.dt.minute,\n","            \"is_holiday\": is_holiday(time_range),\n","        }\n","    )\n","    tmp_df = tmp_df.merge(pd.Series(ntu_snos, name=\"sno\"), how=\"cross\")\n","    tmp_df = tmp_df.merge(station_sno_df, how=\"left\", on=\"sno\")\n","    tmp_df.set_index([\"time\"] + index_names, inplace=True)\n","    tmp_df = tmp_df.merge(\n","        aggregated_train, left_index=True, right_index=True, how=\"left\"\n","    )\n","    tmp_df[\"group\"] = kmeans.predict(tmp_df[property_names])\n","    return tmp_df"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["def do_train(kmeans, t_df, long_train, n_clusters):\n","    # this is needed for brute force\n","    t_df = t_df.reset_index().merge(long_train, on=index_names + [\"time\"])\n","    group_df = pd.DataFrame({}, index=range(n_clusters))\n","    group_df.index.name = \"group\"\n","    # find best sbi for each group\n","    for grp_id in sorted(t_df[\"group\"].unique()):\n","        ys = t_df[t_df[\"group\"] == grp_id][\"sbi\"].values\n","        tots = t_df[t_df[\"group\"] == grp_id][\"tot\"].values\n","        best_sbi, err = brute(ys, tots)\n","        # best_sbi, err = optimal_median(ys, tots)\n","        # print(\n","        #     f\"group {grp_id}, {best_sbi = }, {err = }\",\n","        # )\n","        group_df.loc[grp_id, \"best_sbi\"] = best_sbi\n","        group_df.loc[grp_id, \"err\"] = err\n","    return t_df, group_df"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["def get_prediction(kmeans, time_range, group_df, aggregated_train):\n","    tmp_df = get_group_assignment_df(kmeans, time_range, aggregated_train)\n","    tmp_df = tmp_df.reset_index().merge(group_df, how=\"left\", on=\"group\")\n","    tmp_df = tmp_df[[\"time\", \"sno\", \"best_sbi\"]]\n","    tmp_df.columns = [\"time\", \"sno\", \"sbi\"]\n","    return tmp_df.sort_values(by=[\"sno\", \"time\"], ignore_index=True)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["def evaluation(y_true, y_pred, df_):\n","    print(\"MAE\", mean_absolute_error(y_true, y_pred))\n","    sarr = station_sno_df.loc[df_[\"sno\"]].values.reshape(-1)\n","    err = (\n","        3\n","        * np.abs((y_pred - y_true) / sarr)\n","        * (np.abs(y_true / sarr - 1 / 3) + np.abs(y_true / sarr - 2 / 3))\n","    )\n","    print(\"Score\", err.mean())"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["test_true = (\n","    test.resample(\"20min\")\n","    .agg(\"first\")\n","    .reset_index()\n","    .melt(id_vars=\"time\", var_name=\"sno\", value_name=\"sbi\")\n","    .sort_values(by=[\"sno\", \"time\"], ignore_index=True)\n",")\n","test_true[\"sbi\"] = test_true[\"sbi\"].bfill()\n","test_true.set_index(\"time\", inplace=True)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["long_train, aggregated_train, to_group_df = prepare_data(train)"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["to_group_df has NaN\n"]}],"source":["if long_train.isnull().any().any():\n","    print(\"long_train has NaN\")\n","if aggregated_train.isnull().any().any():\n","    print(\"aggregated_train has NaN\")\n","if to_group_df.isnull().any().any():\n","    print(\"to_group_df has NaN\")\n","    to_group_df.dropna(inplace=True)"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["========================================\n","n_clusters = 2000\n","count    2000.000000\n","mean      271.712500\n","std       303.064448\n","min         9.000000\n","25%        61.750000\n","50%       170.000000\n","75%       371.250000\n","max      2560.000000\n","dtype: float64\n","count    2000.000000\n","mean        0.295640\n","std         0.148255\n","min         0.014522\n","25%         0.182492\n","50%         0.274853\n","75%         0.388088\n","max         0.880235\n","Name: err, dtype: float64\n","                           sno   sbi\n","time                                \n","2023-12-01 00:00:00  500101001  13.0\n","2023-12-01 00:20:00  500101001  11.0\n","2023-12-01 00:40:00  500101001  11.0\n","2023-12-01 01:00:00  500101001  12.0\n","2023-12-01 01:20:00  500101001   9.0\n","...                        ...   ...\n","2023-12-07 22:20:00  500119091   1.0\n","2023-12-07 22:40:00  500119091   1.0\n","2023-12-07 23:00:00  500119091   1.0\n","2023-12-07 23:20:00  500119091   1.0\n","2023-12-07 23:40:00  500119091   1.0\n","\n","[55944 rows x 2 columns]\n"]}],"source":["n_clusters = 2000\n","print(\"=\" * 40)\n","print(f\"{n_clusters = }\")\n","km = KMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\")\n","to_group_df[\"group\"] = km.fit_predict(to_group_df[property_names])\n","t_df = get_group_assignment_df(km, train.index, aggregated_train)\n","t_df, group_df = do_train(km, t_df, long_train, n_clusters)\n","print(t_df.groupby(\"group\").size().describe())\n","print(group_df[\"err\"].describe())\n","test_pred = get_prediction(km, test_true.index, group_df, aggregated_train)\n","test_pred.set_index(\"time\", inplace=True)\n","#print(test_pred)\n","# evaluation(\n","#    test_true[\"sbi\"].values,\n","#    test_pred[\"sbi\"].values,\n","#    test_true,\n","# )"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["sno                  500101001  500101002  500101003  500101004  500101005  \\\n","time                                                                         \n","2023-12-01 00:00:00       13.0        4.0        5.0        4.0        2.0   \n","2023-12-01 00:20:00       11.0        5.0        8.0        6.0        2.0   \n","2023-12-01 00:40:00       11.0        2.0        5.0        6.0        2.0   \n","2023-12-01 01:00:00       12.0        4.0        5.0        6.0        3.0   \n","2023-12-01 01:20:00        9.0        4.0        6.0        6.0        3.0   \n","...                        ...        ...        ...        ...        ...   \n","2023-12-07 22:20:00        2.0        0.0        2.0        2.0        1.0   \n","2023-12-07 22:40:00        3.0        0.0        3.0        1.0        1.0   \n","2023-12-07 23:00:00       14.0        2.0        3.0        2.0        1.0   \n","2023-12-07 23:20:00       16.0        2.0        3.0        2.0        1.0   \n","2023-12-07 23:40:00       17.0        2.0        5.0        4.0        2.0   \n","\n","sno                  500101006  500101007  500101008  500101009  500101010  \\\n","time                                                                         \n","2023-12-01 00:00:00        0.0        1.0        5.0        2.0        2.0   \n","2023-12-01 00:20:00        0.0        1.0        5.0        2.0        3.0   \n","2023-12-01 00:40:00        0.0        1.0        5.0        3.0        5.0   \n","2023-12-01 01:00:00        1.0        1.0        5.0        3.0        5.0   \n","2023-12-01 01:20:00        1.0        1.0        5.0        3.0        5.0   \n","...                        ...        ...        ...        ...        ...   \n","2023-12-07 22:20:00        0.0        0.0        5.0        1.0        1.0   \n","2023-12-07 22:40:00        0.0        0.0        5.0        1.0        2.0   \n","2023-12-07 23:00:00        0.0        0.0        5.0        1.0        3.0   \n","2023-12-07 23:20:00        0.0        1.0        5.0        2.0        2.0   \n","2023-12-07 23:40:00        0.0        1.0        5.0        2.0        2.0   \n","\n","sno                  ...  500119082  500119083  500119084  500119085  \\\n","time                 ...                                               \n","2023-12-01 00:00:00  ...        1.0        2.0        0.0        0.0   \n","2023-12-01 00:20:00  ...        2.0        4.0        0.0        1.0   \n","2023-12-01 00:40:00  ...        2.0        4.0        0.0        1.0   \n","2023-12-01 01:00:00  ...        2.0        4.0        0.0        3.0   \n","2023-12-01 01:20:00  ...        6.0        2.0        0.0        3.0   \n","...                  ...        ...        ...        ...        ...   \n","2023-12-07 22:20:00  ...        0.0        0.0        0.0        0.0   \n","2023-12-07 22:40:00  ...        0.0        0.0        0.0        0.0   \n","2023-12-07 23:00:00  ...        0.0        0.0        0.0        0.0   \n","2023-12-07 23:20:00  ...        0.0        0.0        0.0        0.0   \n","2023-12-07 23:40:00  ...        0.0        3.0        0.0        0.0   \n","\n","sno                  500119086  500119087  500119088  500119089  500119090  \\\n","time                                                                         \n","2023-12-01 00:00:00        0.0        2.0        1.0        1.0        7.0   \n","2023-12-01 00:20:00        0.0        1.0        1.0        1.0        7.0   \n","2023-12-01 00:40:00        1.0        1.0        2.0        5.0        7.0   \n","2023-12-01 01:00:00        1.0        1.0        2.0        5.0        7.0   \n","2023-12-01 01:20:00        1.0        1.0        2.0        5.0        7.0   \n","...                        ...        ...        ...        ...        ...   \n","2023-12-07 22:20:00        0.0        0.0        1.0        0.0        0.0   \n","2023-12-07 22:40:00        0.0        0.0        0.0        0.0        1.0   \n","2023-12-07 23:00:00        0.0        1.0        1.0        0.0        1.0   \n","2023-12-07 23:20:00        1.0        1.0        0.0        0.0        6.0   \n","2023-12-07 23:40:00        1.0        1.0        1.0        0.0        6.0   \n","\n","sno                  500119091  \n","time                            \n","2023-12-01 00:00:00        1.0  \n","2023-12-01 00:20:00        1.0  \n","2023-12-01 00:40:00        1.0  \n","2023-12-01 01:00:00        1.0  \n","2023-12-01 01:20:00        1.0  \n","...                        ...  \n","2023-12-07 22:20:00        1.0  \n","2023-12-07 22:40:00        1.0  \n","2023-12-07 23:00:00        1.0  \n","2023-12-07 23:20:00        1.0  \n","2023-12-07 23:40:00        1.0  \n","\n","[504 rows x 112 columns]\n"]}],"source":["# pivot test_pred by sno to columns\n","test_pred_p = test_pred.pivot(columns=\"sno\", values=\"sbi\")\n","test_pred_p = test_pred_p.sort_index(axis=0)\n","for e in never_active_snos:\n","    test_pred_p[e] = 0\n","\n","# ensure that time index is sorted\n","# sort test_pred_p by sno , don't use ntu_snos because some stations are never active\n","test_pred_p = test_pred_p[sorted(test_pred_p.columns)]\n","\n","print(test_pred_p)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":2}
