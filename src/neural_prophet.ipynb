{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from neuralprophet import NeuralProphet\n","from datetime import datetime, date, time, timedelta\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_absolute_error\n","from utils import *\n","from median_optimization import optimal_median"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["TRAIN_START = \"2023-10-02 00:00\"\n","TRAIN_END = \"2023-12-17 23:59\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["TEST_START = \"2023-12-01 00:00\"\n","TEST_END = \"2023-12-14 23:59\"\n","test_rain_dates = []"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["PUBLIC_START = \"2023-10-21 00:00\"\n","PUBLIC_END = \"2023-10-24 23:59\"\n","public_rain_dates = []"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["PRIVATE_START = \"2023-12-18 00:00\"\n","PRIVARE_END = \"2023-12-24 23:59\"\n","private_rain_dates = [date(2023, 12, 19), date(2023, 12, 20)]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open(\"./cache/small_data_cache.pkl\", \"rb\") as f:\n","    df = pd.read_pickle(f)\n","with open(\"../html.2023.final.data/sno_test_set.txt\") as f:\n","    ntu_snos = [l.strip() for l in f.read().splitlines()]\n","with open(\"./cache/10-03_12_09_rain.pkl\", \"rb\") as f:\n","    rain_df = pd.read_pickle(f)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ntu_tots = get_tot(df, ntu_snos)\n","# the data looks like this:\n","\"\"\"\n","     datetime               sno      tot   sbi   bemp  act\n","0    2023-10-02 00:00:00  500101001  28.0  12.0  16.0   1\n","1    2023-10-02 00:01:00  500101001  28.0  12.0  16.0   1\n","2    2023-10-02 00:02:00  500101001  28.0  13.0  15.0   1\n","...\n","\"\"\"\n","holidays = [d for d in date_range(start=TRAIN_START, end=PRIVARE_END) if is_holiday(d)]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[\"datehour\"] = df[\"time\"].dt.floor(\"H\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rain_df.rename(columns={\"datetime\": \"datehour\"}, inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","argument rain to df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = df.merge(rain_df, on=\"datehour\", how=\"left\")\n","df[\"rain\"].fillna(0, inplace=True)\n","# print(df)\n","df.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["morning_filter = (df[\"datehour\"].dt.hour >= 7) & (df[\"datehour\"].dt.hour <= 21)\n","rain_hours = df[\"datehour\"][\n","    (df[\"sno\"] == \"500101001\") & morning_filter & (df[\"rain\"] >= 0.3)\n","].drop_duplicates()  # 0.3 for dribble , 0.5 for small rain\n","rain_dates = rain_hours.dt.date.drop_duplicates()"]},{"cell_type":"markdown","metadata":{},"source":["print(rain_dates.describe()) # 0.3: 28, 0.5: 15 total:63-62 days"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["date_rain_hour_cnt = rain_hours.dt.date.value_counts()\n","# print(date_rain_hour_cnt)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["long_rain_dates = rain_dates[\n","    rain_dates.isin(date_rain_hour_cnt.index[date_rain_hour_cnt >= 7])\n","]\n","# print(long_rain_dates) # 12\n","# print(long_rain_dates.describe())\n","rainy_dates = long_rain_dates.array  # 12\n","# print(rainy_dates)"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","[ datetime.date(2023, 10, 3),  datetime.date(2023, 10, 4),<br>\n","  datetime.date(2023, 10, 5),  datetime.date(2023, 10, 6),<br>\n","  datetime.date(2023, 10, 8), datetime.date(2023, 10, 20),<br>\n"," datetime.date(2023, 10, 28), datetime.date(2023, 11, 16),<br>\n"," datetime.date(2023, 11, 26), datetime.date(2023, 11, 30),<br>\n","  datetime.date(2023, 12, 4),  datetime.date(2023, 12, 6)]"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","replace rain to is rainy day"]},{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[\"rain\"] = df[\"time\"].dt.date.isin(rainy_dates).astype(np.float64)"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","delete rain processing variables"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["del rain_df\n","del morning_filter\n","del rain_hours, rain_dates\n","del date_rain_hour_cnt\n","del long_rain_dates"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","Only one main table. Then always slice from it"]},{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.drop(columns=[\"tot\", \"datehour\", \"act\"], inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.rename(columns={\"time\": \"ds\", \"sbi\": \"y\"}, inplace=True)\n","# print(df)"]},{"cell_type":"markdown","metadata":{},"source":["%%<br>\n","df['holiday'] = df['time'].dt.date.isin(holidays)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["holidays_df = pd.DataFrame(\n","    {\n","        \"event\": \"holiday\",\n","        \"ds\": pd.to_datetime(holidays),\n","    }\n",")"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","Training about 12min (no rain/holiday) 17min (rain/holiday)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import concurrent.futures"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train = df[df[\"ds\"].dt.date.isin(date_range(TRAIN_START, TRAIN_END))].copy()\n","pred_dfs = {}\n","models = {}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_df = pd.DataFrame({\"ds\": pd.date_range(TEST_START, TEST_END, freq=\"20min\")})\n","# test_df['y'] = None\n","test_df[\"rain\"] = test_df[\"ds\"].dt.date.isin(rainy_dates).astype(np.float64)\n","test_df[\"holiday\"] = test_df[\"ds\"].dt.date.isin(holidays).astype(np.float64)\n","print(test_df)"]},{"cell_type":"markdown","metadata":{},"source":["Function to train and predict for a single station"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_and_predict(sno):\n","    station_train = (\n","        train[train[\"sno\"] == sno]\n","        .resample(\"5min\", on=\"ds\")\n","        .first()\n","        .dropna()\n","        .reset_index()\n","    )\n","    m = NeuralProphet()\n","    m = m.add_events(\"holiday\")  # , lower_window=0, upper_window=1)\n","    m.add_future_regressor(\"rain\")\n","    station_train = m.create_df_with_events(station_train, holidays_df)  # float 0 1\n","    # print(station_train)\n","    sno_df = df[df[\"sno\"] == sno]\n","    # print(sno_df)\n","    # print(test_df)\n","    sno_test_df = test_df.merge(sno_df[[\"ds\", \"y\"]], on=\"ds\", how=\"left\")\n","    # print(sno_test_df['y'])\n","    m.fit(station_train[[\"ds\", \"y\", \"rain\", \"holiday\"]])\n","\n","    # test_df['y'] = sno_df[sno_df[\"ds\"].isin(test_df[\"ds\"])][\"y\"].values\n","    forecast = m.predict(sno_test_df)\n","    return sno, forecast, m"]},{"cell_type":"markdown","metadata":{},"source":["sno, forecast, m = train_and_predict(ntu_snos[0])"]},{"cell_type":"markdown","metadata":{},"source":["m.plot(forecast)"]},{"cell_type":"markdown","metadata":{},"source":["Train and predict for each station in parallel"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with concurrent.futures.ProcessPoolExecutor() as executor:  # seems to be single thread\n","    # Submit jobs for each station\n","    future_to_sno = {executor.submit(train_and_predict, sno): sno for sno in ntu_snos}\n","\n","    # Retrieve results as they become available\n","    for future in concurrent.futures.as_completed(future_to_sno):\n","        sno = future_to_sno[future]\n","        try:\n","            result_sno, forecast_sno, model_sno = future.result()\n","            pred_dfs[result_sno] = forecast_sno\n","            models[result_sno] = model_sno\n","        except Exception as e:\n","            print(f\"Error processing station {sno}: {e}\")"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","print(pred_dfs[ntu_snos[2]])"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","pred_df = pred_dfs[ntu_snos[0]]<br>\n","m = models[ntu_snos[0]]<br>\n","fig = m.plot_components(pred_df)<br>\n","plt.savefig(f\"./prophet_lines/{ntu_snos[0]}_components.png\")<br>\n","plt.close(fig)<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_range = pd.date_range(TEST_START, TEST_END, freq=\"20min\")\n","test_len = len(list(test_range))\n","test_df = df[df[\"ds\"].isin(test_range)]\n","test_tb = (\n","    pd.pivot_table(test_df, index=\"ds\", columns=\"sno\", values=\"y\")\n","    .resample(\"20min\")\n","    .first()\n","    .bfill()\n","    .ffill()\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_pred = np.empty([test_len, 0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["errors = {}\n","for sno, tot in zip(ntu_snos, ntu_tots):\n","    pred_df = pred_dfs[sno]\n","    m = models[sno]\n","    fig = m.plot_components(pred_df)\n","    plt.savefig(f\"./prophet_lines/{sno}_components.png\")\n","    plt.close(fig)\n","    pred_df[\"yhat\"].clip(lower=0, upper=tot, inplace=True)\n","    pred = pred_df[\"yhat\"].to_numpy()\n","    y_pred = np.column_stack((y_pred, pred))\n","\n","    # TODO E_in\n","    ans = test_tb[sno]\n","    # print(ans.shape, pred_df.shape)\n","    err = error(ans.to_numpy(), pred, np.full(test_len, tot))\n","    errors[sno] = err\n","    ax = pred_df.plot(x=\"ds\", y=\"yhat\", figsize=(20, 6), title=f\"score: {err}\")\n","    ans.plot(ax=ax, x=\"ds\", y=\"y\")\n","    plt.savefig(f\"./neural_prophet_lines/{sno}.png\")\n","    plt.close()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open(\"./neural_prophet_lines/results.txt\", \"w\") as f:\n","    for e in sorted(errors.items(), key=lambda x: x[1]):\n","        f.write(f\"{e[0]}: {e[1]}\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","Self evaluation (Test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test = test_tb[test_tb.index.to_series().dt.date.isin(date_range(TEST_START, TEST_END))]\n","y_test = test.values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print_time_ranges(TRAIN_START, TRAIN_END, TEST_START, TEST_END)\n","assert y_test.shape == y_pred.shape, \"test pred shape not matched\"\n","# y_pred = y_pred[:,1:]\n","print(y_test.shape)  # (1008, 112)\n","print(y_pred.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["evaluation(y_test, y_pred, ntu_tots, test_range)"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","<br>\n","all sunny \\<br>\n","MAE:  0.13275823190272154 \\<br>\n","Score:  0.24003751756886713<br>\n","<br>\n","all data \\<br>\n","MAE:  0.1229537906365303 \\<br>\n","Score:  0.21939612130515754<br>\n","<br>\n","rain+sunny \\<br>\n","MAE:  0.12230290642321821 \\<br>\n","Score:  0.2167087889314868<br>\n"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","does the same at public test set (2023/10/21 - 2023/10/24)"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","public_test_range = pd.date_range(PUBLIC_START, PUBLIC_END, freq=\"20min\")<br>\n","# list makes indexer 1D, or it is an 2D indexer<br>\n","ftr = list(<br>\n","    np.stack([[False]*(4*72),public_test_range.time, public_test_range.weekday]).T<br>\n",")<br>\n","y_public_df = result_df.loc[ftr]<br>\n","#print(\"y_public_df Before\")<br>\n","#print(y_public_df)"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","Check public"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","#print(\"y_public_df After\")<br>\n","#print(y_public_df)"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","for col, tot in zip(y_public_df.columns, ntu_tots):<br>\n","    y_public_df[col] = y_public_df[col].clip(lower=0, upper=tot)<br>\n","y_public_test = y_public_df.values<br>\n","public_test_df = pd.DataFrame(y_public_test, columns=ntu_snos, index=public_test_range)"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","private_test_range = pd.date_range(PRIVATE_START, PRIVARE_END, freq=\"20min\")<br>\n","ftr = list(<br>\n","    np.stack(<br>\n","        [[d in private_rain_dates for d in private_test_range.date],private_test_range.time, private_test_range.weekday]<br>\n","    ).T<br>\n",")<br>\n","y_private_df = result_df.loc[ftr]"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","# TODO patch private<br>\n","# Set the initial time<br>\n","current_datetime = pd.to_datetime(\"2023-12-17 22:40\")<br>\n","current_time = current_datetime.time()<br>\n","cur_data = tb[tb.index == current_datetime]<br>\n","print(cur_data)<br>\n","cur = tb[tb.index.to_series().dt.time == current_time]<br>\n","<br>\n","# Loop to fetch data for the next 20 minutes<br>\n","end_datetime = pd.to_datetime(\"2023-12-18 04:00\")<br>\n","next_datetime = current_datetime + timedelta(minutes=60)<br>\n","td = timedelta(minutes=20)<br>\n","total_td = timedelta(minutes=60)<br>\n","while next_datetime <= end_datetime:<br>\n","    # Increment current_time by 20 minutes<br>\n","    next_datetime += td<br>\n","    next_time = next_datetime.time()<br>\n","    total_td += td<br>\n","    # Filter data from tb for the current time<br>\n","    nxt = tb[tb.index.to_series().dt.time == next_time]<br>\n","    diff = nxt - cur.shift(freq=total_td)<br>\n","    mean_diff = pd.pivot_table(diff.mean().reset_index(), columns=\"sno\")<br>\n","    mean_diff.set_index(cur_data.index, inplace=True)<br>\n","    upd = cur_data + mean_diff<br>\n","    print(upd)<br>\n","    patch_datetime = next_datetime<br>\n","    patch_time = patch_datetime.time()<br>\n","    print(patch_time)<br>\n","    upd.set_index([[patch_time], [0]], inplace=True)<br>\n","    y_private_df.loc[(patch_time, 0)] = upd"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","if y_private_df.isnull().values.any():<br>\n","    print(\"DataFrame contains NaN values.\")<br>\n","print(y_private_df)"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","assert not y_private_df.isnull().values.any(), \"private contains null\"<br>\n","for col, tot in zip(y_private_df.columns, ntu_tots):<br>\n","    y_private_df[col] = y_private_df[col].clip(lower=0, upper=tot)<br>\n","y_private_test = y_private_df.values<br>\n","private_test_df = pd.DataFrame(<br>\n","    y_private_test, columns=ntu_snos, index=private_test_range<br>\n",")"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","tmp = pd.concat(<br>\n","    [<br>\n","        public_test_df,<br>\n","        private_test_df,<br>\n","    ]<br>\n",")<br>\n","# reset_index: old index => \"time\" column<br>\n","# id_vars: fixed column like index<br>\n","# var_name: columns.name to \"sno\" column<br>\n","# value_name: value => \"sbi\" column<br>\n","tmp = tmp.reset_index(names=\"time\").melt(<br>\n","    id_vars=\"time\", var_name=\"sno\", value_name=\"sbi\"<br>\n",")<br>\n","out_df = pd.DataFrame(<br>\n","    {<br>\n","        \"id\": (<br>\n","            tmp[\"time\"].dt.strftime(\"%Y%m%d\")<br>\n","            + \"_\"<br>\n","            + tmp[\"sno\"]<br>\n","            + \"_\"<br>\n","            + tmp[\"time\"].dt.strftime(\"%H:%M\")<br>\n","        ),<br>\n","        \"sbi\": tmp[\"sbi\"],<br>\n","    }<br>\n",")<br>\n","out_df.to_csv(<br>\n","    f\"../submission/pub_pri_{datetime.now().strftime('%m-%d-%H-%M')}.csv\", index=False<br>\n",")<br>\n","print(\"csv created\")"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","TODO patch private<br><br>\n","Set the initial time"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","<br>\n","<br><br>\n","current_datetime = pd.to_datetime(\"2023-12-10 23:40\")<br><br>\n","current_time = current_datetime.time()<br><br>\n","cur_data = old_tb[old_tb.index == current_datetime]<br><br>\n","print(cur_data)<br><br>\n","cur = old_tb[old_tb.index.to_series().dt.time == current_time]<br><br>\n","# Loop to fetch data for the next 20 minutes<br><br>\n","end_datetime = pd.to_datetime(\"2023-12-04 03:59\")<br><br>\n","next_datetime = current_datetime<br><br>\n","td = timedelta(minutes=20)<br><br>\n","total_td = timedelta(minutes=0)<br><br>\n","while next_datetime <= end_datetime:<br><br>\n","    # Increment current_time by 20 minutes<br><br>\n","    next_datetime += td<br><br>\n","    next_time = next_datetime.time()<br><br>\n","    total_td += td<br><br>\n","    # Filter data from old_tb for the current time<br><br>\n","    nxt = old_tb[old_tb.index.to_series().dt.time == next_time]<br><br>\n","    diff = nxt - cur.shift(freq=total_td)<br><br>\n","    mean_diff = pd.pivot_table(diff.mean().reset_index(), columns=\"sno\")<br><br>\n","    mean_diff.set_index(cur_data.index, inplace=True)<br><br>\n","    upd = cur_data + mean_diff<br><br>\n","    # print(upd)<br><br>\n","    patch_datetime = next_datetime + timedelta(minutes=1)<br><br>\n","    patch_time = patch_datetime.time()<br><br>\n","    upd.set_index([[patch_time], [0]], inplace=True)<br><br>\n","    y_private_df.loc[(patch_time, 0)] = upd<br><br>\n","print(y_private_df)<br><br>\n"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","convert the prediction to the required format"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":2}
