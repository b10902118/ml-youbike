{"cells":[{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["from neuralprophet import NeuralProphet\n","from datetime import datetime, date, time, timedelta\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_absolute_error\n","from utils import *\n","from median_optimization import optimal_median"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["TRAIN_START = \"2023-10-02 00:00\"\n","TRAIN_END = \"2023-12-17 23:59\""]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["TEST_START = \"2023-12-01 00:00\"\n","TEST_END = \"2023-12-14 23:59\"\n","test_rain_dates = []"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["PUBLIC_START = \"2023-10-21 00:00\"\n","PUBLIC_END = \"2023-10-24 23:59\"\n","public_rain_dates = []"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["PRIVATE_START = \"2023-12-18 00:00\"\n","PRIVARE_END = \"2023-12-24 23:59\"\n","private_rain_dates = [date(2023, 12, 19), date(2023, 12, 20)]"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["with open(\"./cache/small_data_cache.pkl\", \"rb\") as f:\n","    df = pd.read_pickle(f)\n","with open(\"../html.2023.final.data/sno_test_set.txt\") as f:\n","    ntu_snos = [l.strip() for l in f.read().splitlines()]\n","with open(\"./cache/10-03_12_09_rain.pkl\", \"rb\") as f:\n","    rain_df = pd.read_pickle(f)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["ntu_tots = get_tot(df, ntu_snos)\n","# the data looks like this:\n","\"\"\"\n","     datetime               sno      tot   sbi   bemp  act\n","0    2023-10-02 00:00:00  500101001  28.0  12.0  16.0   1\n","1    2023-10-02 00:01:00  500101001  28.0  12.0  16.0   1\n","2    2023-10-02 00:02:00  500101001  28.0  13.0  15.0   1\n","...\n","\"\"\"\n","holidays = [d for d in date_range(start=TRAIN_START, end=PRIVARE_END) if is_holiday(d)]"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["df[\"datehour\"] = df[\"time\"].dt.floor(\"H\")"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["rain_df.rename(columns={\"datetime\": \"datehour\"}, inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","argument rain to df"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tot</th>\n","      <th>sbi</th>\n","      <th>rain</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>1.105241e+07</td>\n","      <td>1.105241e+07</td>\n","      <td>1.105241e+07</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>2.578958e+01</td>\n","      <td>7.514006e+00</td>\n","      <td>1.390311e-01</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>1.701789e+01</td>\n","      <td>1.029793e+01</td>\n","      <td>6.258437e-01</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>5.000000e+00</td>\n","      <td>0.000000e+00</td>\n","      <td>0.000000e+00</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>1.500000e+01</td>\n","      <td>1.000000e+00</td>\n","      <td>0.000000e+00</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>2.000000e+01</td>\n","      <td>4.000000e+00</td>\n","      <td>0.000000e+00</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>3.000000e+01</td>\n","      <td>1.100000e+01</td>\n","      <td>0.000000e+00</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>9.900000e+01</td>\n","      <td>9.900000e+01</td>\n","      <td>9.000000e+00</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                tot           sbi          rain\n","count  1.105241e+07  1.105241e+07  1.105241e+07\n","mean   2.578958e+01  7.514006e+00  1.390311e-01\n","std    1.701789e+01  1.029793e+01  6.258437e-01\n","min    5.000000e+00  0.000000e+00  0.000000e+00\n","25%    1.500000e+01  1.000000e+00  0.000000e+00\n","50%    2.000000e+01  4.000000e+00  0.000000e+00\n","75%    3.000000e+01  1.100000e+01  0.000000e+00\n","max    9.900000e+01  9.900000e+01  9.000000e+00"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["df = df.merge(rain_df, on=\"datehour\", how=\"left\")\n","df[\"rain\"].fillna(0, inplace=True)\n","# print(df)\n","df.describe()"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["morning_filter = (df[\"datehour\"].dt.hour >= 7) & (df[\"datehour\"].dt.hour <= 21)\n","rain_hours = df[\"datehour\"][\n","    (df[\"sno\"] == \"500101001\") & morning_filter & (df[\"rain\"] >= 0.3)\n","].drop_duplicates()  # 0.3 for dribble , 0.5 for small rain\n","rain_dates = rain_hours.dt.date.drop_duplicates()"]},{"cell_type":"markdown","metadata":{},"source":["print(rain_dates.describe()) # 0.3: 28, 0.5: 15 total:63-62 days"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["date_rain_hour_cnt = rain_hours.dt.date.value_counts()\n","# print(date_rain_hour_cnt)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["long_rain_dates = rain_dates[\n","    rain_dates.isin(date_rain_hour_cnt.index[date_rain_hour_cnt >= 7])\n","]\n","# print(long_rain_dates) # 12\n","# print(long_rain_dates.describe())\n","rainy_dates = long_rain_dates.array  # 12\n","# print(rainy_dates)"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","[ datetime.date(2023, 10, 3),  datetime.date(2023, 10, 4),<br>\n","  datetime.date(2023, 10, 5),  datetime.date(2023, 10, 6),<br>\n","  datetime.date(2023, 10, 8), datetime.date(2023, 10, 20),<br>\n"," datetime.date(2023, 10, 28), datetime.date(2023, 11, 16),<br>\n"," datetime.date(2023, 11, 26), datetime.date(2023, 11, 30),<br>\n","  datetime.date(2023, 12, 4),  datetime.date(2023, 12, 6)]"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","replace rain to is rainy day"]},{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["df[\"rain\"] = df[\"time\"].dt.date.isin(rainy_dates).astype(np.float64)"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","delete rain processing variables"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["del rain_df\n","del morning_filter\n","del rain_hours, rain_dates\n","del date_rain_hour_cnt\n","del long_rain_dates"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","Only one main table. Then always slice from it"]},{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["df.drop(columns=[\"tot\", \"datehour\", \"act\"], inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["%%"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["df.rename(columns={\"time\": \"ds\", \"sbi\": \"y\"}, inplace=True)\n","# print(df)"]},{"cell_type":"markdown","metadata":{},"source":["%%<br>\n","df['holiday'] = df['time'].dt.date.isin(holidays)"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["holidays_df = pd.DataFrame(\n","    {\n","        \"event\": \"holiday\",\n","        \"ds\": pd.to_datetime(holidays),\n","    }\n",")"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","Training about 12min (no rain/holiday) 17min (rain/holiday)"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["import concurrent.futures"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["train = df[df[\"ds\"].dt.date.isin(date_range(TRAIN_START, TRAIN_END))].copy()\n","pred_dfs = {}\n","models = {}"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                      ds  rain  holiday\n","0    2023-12-01 00:00:00   1.0      0.0\n","1    2023-12-01 00:20:00   1.0      0.0\n","2    2023-12-01 00:40:00   1.0      0.0\n","3    2023-12-01 01:00:00   1.0      0.0\n","4    2023-12-01 01:20:00   1.0      0.0\n","...                  ...   ...      ...\n","1003 2023-12-14 22:20:00   0.0      0.0\n","1004 2023-12-14 22:40:00   0.0      0.0\n","1005 2023-12-14 23:00:00   0.0      0.0\n","1006 2023-12-14 23:20:00   0.0      0.0\n","1007 2023-12-14 23:40:00   0.0      0.0\n","\n","[1008 rows x 3 columns]\n"]}],"source":["test_df = pd.DataFrame({\"ds\": pd.date_range(TEST_START, TEST_END, freq=\"20min\")})\n","# test_df['y'] = None\n","test_df[\"rain\"] = test_df[\"ds\"].dt.date.isin(rainy_dates).astype(np.float64)\n","test_df[\"holiday\"] = test_df[\"ds\"].dt.date.isin(holidays).astype(np.float64)\n","print(test_df)"]},{"cell_type":"markdown","metadata":{},"source":["Function to train and predict for a single station"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["def train_and_predict(sno):\n","    station_train = (\n","        train[train[\"sno\"] == sno]\n","        .resample(\"5min\", on=\"ds\")\n","        .first()\n","        .dropna()\n","        .reset_index()\n","    )\n","    m = NeuralProphet()\n","    m = m.add_events(\"holiday\")  # , lower_window=0, upper_window=1)\n","    m.add_future_regressor(\"rain\") # test\n","    station_train = m.create_df_with_events(station_train, holidays_df)  # float 0 1\n","    # print(station_train)\n","    sno_df = df[df[\"sno\"] == sno]\n","    # print(sno_df)\n","    # print(test_df)\n","    sno_test_df = test_df.merge(sno_df[[\"ds\", \"y\"]], on=\"ds\", how=\"left\")\n","    # print(sno_test_df['y'])\n","    m.fit(station_train[[\"ds\", \"y\", \"rain\", \"holiday\"]])\n","\n","    # test_df['y'] = sno_df[sno_df[\"ds\"].isin(test_df[\"ds\"])][\"y\"].values\n","    forecast = m.predict(sno_test_df)\n","    fig = m.plot_components(forecast)\n","    fig.write_image(f\"./neural_prophet_lines/{sno}_components.png\")\n","    return forecast #sno, forecast, m"]},{"cell_type":"markdown","metadata":{},"source":["sno, forecast, m = train_and_predict(ntu_snos[0])"]},{"cell_type":"markdown","metadata":{},"source":["m.plot(forecast)"]},{"cell_type":"markdown","metadata":{},"source":["Train and predict for each station in parallel"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n","WARNING - (NP.forecaster.fit) - When Global modeling with local normalization, metrics are displayed in normalized scale.\n","INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n","WARNING - (NP.forecaster.fit) - When Global modeling with local normalization, metrics are displayed in normalized scale.\n","INFO - (NP.df_utils._infer_frequency) - Major frequency 5T corresponds to 99.909% of the data.\n","INFO - (NP.df_utils._infer_frequency) - Dataframe freq automatically defined as 5T\n","INFO - (NP.df_utils._infer_frequency) - Major frequency 5T corresponds to 99.909% of the data.\n","INFO - (NP.df_utils._infer_frequency) - Dataframe freq automatically defined as 5T\n","INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n","INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n","INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n","INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\n","INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n","INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 86\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"77039054be134f6da8f77d2b1bc18ebc","version_major":2,"version_minor":0},"text/plain":["Finding best initial lr:   0%|          | 0/257 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 64\n","INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 86\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7577cc79681845a79e0711b3cfee7be1","version_major":2,"version_minor":0},"text/plain":["Finding best initial lr:   0%|          | 0/257 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"19fc4738c7dc4d0cadba77fd36a4f640","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"da6bed3dbdee49988ba557ab2e1d0fcf","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["INFO - (NP.df_utils._infer_frequency) - Major frequency 20T corresponds to 99.901% of the data.\n","WARNING - (NP.df_utils._infer_frequency) - Defined frequency 5T is different than major frequency 20T\n","INFO - (NP.df_utils._infer_frequency) - Major frequency 20T corresponds to 99.901% of the data.\n","WARNING - (NP.df_utils._infer_frequency) - Defined frequency 5T is different than major frequency 20T\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a2dbfe68436f4fa4942ca1dca109859d","version_major":2,"version_minor":0},"text/plain":["Predicting: 309it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n","INFO - (NP.df_utils._infer_frequency) - Major frequency 20T corresponds to 99.901% of the data.\n","WARNING - (NP.df_utils._infer_frequency) - Defined frequency 5T is different than major frequency 20T\n","INFO - (NP.df_utils._infer_frequency) - Major frequency 20T corresponds to 99.901% of the data.\n","WARNING - (NP.df_utils._infer_frequency) - Defined frequency 5T is different than major frequency 20T\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"41359e4aaa5d40cfb61c1f454b6d6feb","version_major":2,"version_minor":0},"text/plain":["Predicting: 309it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n"]}],"source":["with concurrent.futures.ProcessPoolExecutor(max_workers=50) as executor:  # seems to be single thread\n","    # Submit jobs for each station\n","    future_to_sno = {executor.submit(train_and_predict, sno): sno for sno in [ntu_snos[0], ntu_snos[1]]} #ntu_snos}\n","\n","    # Retrieve results as they become available\n","    for future in concurrent.futures.as_completed(future_to_sno):\n","        sno = future_to_sno[future]\n","        try:\n","            #result_sno, forecast_sno, model_sno = future.result()\n","            forecast_sno = future.result()\n","            pred_dfs[sno] = forecast_sno\n","            #models[result_sno] = model_sno\n","        except Exception as e:\n","            print(f\"Error processing station {sno}: {e}\")\n","            exit(1)"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Index(['ds', 'y', 'yhat1', 'trend', 'season_weekly', 'season_daily',\n","       'events_additive', 'event_holiday', 'future_regressors_additive',\n","       'future_regressor_rain'],\n","      dtype='object')\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1c2312778a2c43098268337132141d93","version_major":2,"version_minor":0},"text/plain":["FigureWidgetResampler({\n","    'data': [{'fill': 'none',\n","              'line': {'color': 'rgba(45, 146, 255, 1.0)', 'width': 2},\n","              'mode': 'lines',\n","              'name': '<b style=\"color:sandybrown\">[R]</b> yhat1 <i style=\"color:#fc9944\">~20m</i>',\n","              'type': 'scatter',\n","              'uid': '10e9f3d2-6a9a-4b3f-9416-ba5626247e9a',\n","              'x': array([datetime.datetime(2023, 12, 1, 0, 0),\n","                          datetime.datetime(2023, 12, 1, 0, 20),\n","                          datetime.datetime(2023, 12, 1, 0, 40), ...,\n","                          datetime.datetime(2023, 12, 14, 22, 40),\n","                          datetime.datetime(2023, 12, 14, 23, 20),\n","                          datetime.datetime(2023, 12, 14, 23, 40)], dtype=object),\n","              'y': array([11.41356182, 12.10298252, 12.62410355, ...,  6.55064917,  8.28795815,\n","                           9.1481266 ])},\n","             {'marker': {'color': 'black', 'size': 4},\n","              'mode': 'markers',\n","              'name': '<b style=\"color:sandybrown\">[R]</b> Actual <i style=\"color:#fc9944\">~20m</i>',\n","              'type': 'scatter',\n","              'uid': 'b42e4707-dd91-4833-8e15-bb2ba5f4fb73',\n","              'x': array([datetime.datetime(2023, 12, 1, 0, 0),\n","                          datetime.datetime(2023, 12, 1, 0, 20),\n","                          datetime.datetime(2023, 12, 1, 2, 0), ...,\n","                          datetime.datetime(2023, 12, 14, 22, 40),\n","                          datetime.datetime(2023, 12, 14, 23, 0),\n","                          datetime.datetime(2023, 12, 14, 23, 40)], dtype=object),\n","              'y': array([ 5.,  6., nan, ...,  1.,  1., 12.])}],\n","    'layout': {'autosize': True,\n","               'font': {'size': 10},\n","               'height': 420,\n","               'hovermode': 'x unified',\n","               'margin': {'b': 0, 'l': 0, 'pad': 0, 'r': 10, 't': 10},\n","               'showlegend': True,\n","               'template': '...',\n","               'title': {'font': {'size': 12}},\n","               'width': 700,\n","               'xaxis': {'linewidth': 1.5,\n","                         'mirror': True,\n","                         'rangeselector': {'buttons': [{'count': 7, 'label': '1w', 'step': 'day', 'stepmode': 'backward'},\n","                                                       {'count': 1,\n","                                                        'label': '1m',\n","                                                        'step': 'month',\n","                                                        'stepmode': 'backward'},\n","                                                       {'count': 6,\n","                                                        'label': '6m',\n","                                                        'step': 'month',\n","                                                        'stepmode': 'backward'},\n","                                                       {'count': 1, 'label': '1y', 'step': 'year', 'stepmode': 'backward'},\n","                                                       {'step': 'all'}]},\n","                         'rangeslider': {'visible': True},\n","                         'showline': True,\n","                         'title': {'text': 'ds'},\n","                         'type': 'date'},\n","               'yaxis': {'linewidth': 1.5, 'mirror': True, 'showline': True, 'title': {'text': 'y'}}}\n","})"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["print(pred_dfs[ntu_snos[0]].columns)"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","pred_df = pred_dfs[ntu_snos[0]]<br>\n","m = models[ntu_snos[0]]<br>\n","fig = m.plot_components(pred_df)<br>\n","plt.savefig(f\"./prophet_lines/{ntu_snos[0]}_components.png\")<br>\n","plt.close(fig)<br>\n"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["test_range = pd.date_range(TEST_START, TEST_END, freq=\"20min\")\n","test_len = len(list(test_range))\n","test_df = df[df[\"ds\"].isin(test_range)]\n","test_tb = (\n","    pd.pivot_table(test_df, index=\"ds\", columns=\"sno\", values=\"y\")\n","    .resample(\"20min\")\n","    .first()\n","    .bfill()\n","    .ffill()\n",")"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["y_pred = np.empty([test_len, 0])"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["errors = {}\n","#for sno, tot in zip(ntu_snos, ntu_tots):\n","for sno, tot in zip([ntu_snos[0], ntu_snos[1]], [ntu_tots[0], ntu_tots[1]]):\n","    pred_df = pred_dfs[sno]\n","    #print(pred_df)\n","    \n","    #m = models[sno]\n","    #fig = m.plot_components(pred_df)\n","    #plt.savefig(f\"./prophet_lines/{sno}_components.png\")\n","    #plt.close(fig)\n","    pred_df[\"yhat1\"].clip(lower=0, upper=tot, inplace=True)\n","    pred = pred_df[\"yhat1\"].to_numpy()\n","    y_pred = np.column_stack((y_pred, pred))\n","\n","    # TODO E_in\n","    ans = test_tb[sno]\n","    # print(ans.shape, pred_df.shape)\n","    err = error(ans.to_numpy(), pred, np.full(test_len, tot))\n","    errors[sno] = err\n","    ax = pred_df.plot(x=\"ds\", y=\"yhat1\", figsize=(20, 6), title=f\"score: {err}\")\n","    ans.plot(ax=ax, x=\"ds\", y=\"y\")\n","    plt.savefig(f\"./neural_prophet_lines/{sno}.png\")\n","    plt.close()"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["with open(\"./neural_prophet_lines/results.txt\", \"w\") as f:\n","    for e in sorted(errors.items(), key=lambda x: x[1]):\n","        f.write(f\"{e[0]}: {e[1]}\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","Self evaluation (Test)"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["test = test_tb[test_tb.index.to_series().dt.date.isin(date_range(TEST_START, TEST_END))]\n","y_test = test.values"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train: 10/02 - 12/17\n","Test: 12/01 - 12/14\n","(1008, 112)\n","(1008, 2)\n"]}],"source":["print_time_ranges(TRAIN_START, TRAIN_END, TEST_START, TEST_END)\n","assert y_test.shape == y_pred.shape, \"test pred shape not matched\"\n","# y_pred = y_pred[:,1:]\n","print(y_test.shape)  # (1008, 112)\n","print(y_pred.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["evaluation(y_test, y_pred, ntu_tots, test_range)"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","<br>\n","all sunny \\<br>\n","MAE:  0.13275823190272154 \\<br>\n","Score:  0.24003751756886713<br>\n","<br>\n","all data \\<br>\n","MAE:  0.1229537906365303 \\<br>\n","Score:  0.21939612130515754<br>\n","<br>\n","rain+sunny \\<br>\n","MAE:  0.12230290642321821 \\<br>\n","Score:  0.2167087889314868<br>\n"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","does the same at public test set (2023/10/21 - 2023/10/24)"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","public_test_range = pd.date_range(PUBLIC_START, PUBLIC_END, freq=\"20min\")<br>\n","# list makes indexer 1D, or it is an 2D indexer<br>\n","ftr = list(<br>\n","    np.stack([[False]*(4*72),public_test_range.time, public_test_range.weekday]).T<br>\n",")<br>\n","y_public_df = result_df.loc[ftr]<br>\n","#print(\"y_public_df Before\")<br>\n","#print(y_public_df)"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","Check public"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","#print(\"y_public_df After\")<br>\n","#print(y_public_df)"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","for col, tot in zip(y_public_df.columns, ntu_tots):<br>\n","    y_public_df[col] = y_public_df[col].clip(lower=0, upper=tot)<br>\n","y_public_test = y_public_df.values<br>\n","public_test_df = pd.DataFrame(y_public_test, columns=ntu_snos, index=public_test_range)"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","private_test_range = pd.date_range(PRIVATE_START, PRIVARE_END, freq=\"20min\")<br>\n","ftr = list(<br>\n","    np.stack(<br>\n","        [[d in private_rain_dates for d in private_test_range.date],private_test_range.time, private_test_range.weekday]<br>\n","    ).T<br>\n",")<br>\n","y_private_df = result_df.loc[ftr]"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","# TODO patch private<br>\n","# Set the initial time<br>\n","current_datetime = pd.to_datetime(\"2023-12-17 22:40\")<br>\n","current_time = current_datetime.time()<br>\n","cur_data = tb[tb.index == current_datetime]<br>\n","print(cur_data)<br>\n","cur = tb[tb.index.to_series().dt.time == current_time]<br>\n","<br>\n","# Loop to fetch data for the next 20 minutes<br>\n","end_datetime = pd.to_datetime(\"2023-12-18 04:00\")<br>\n","next_datetime = current_datetime + timedelta(minutes=60)<br>\n","td = timedelta(minutes=20)<br>\n","total_td = timedelta(minutes=60)<br>\n","while next_datetime <= end_datetime:<br>\n","    # Increment current_time by 20 minutes<br>\n","    next_datetime += td<br>\n","    next_time = next_datetime.time()<br>\n","    total_td += td<br>\n","    # Filter data from tb for the current time<br>\n","    nxt = tb[tb.index.to_series().dt.time == next_time]<br>\n","    diff = nxt - cur.shift(freq=total_td)<br>\n","    mean_diff = pd.pivot_table(diff.mean().reset_index(), columns=\"sno\")<br>\n","    mean_diff.set_index(cur_data.index, inplace=True)<br>\n","    upd = cur_data + mean_diff<br>\n","    print(upd)<br>\n","    patch_datetime = next_datetime<br>\n","    patch_time = patch_datetime.time()<br>\n","    print(patch_time)<br>\n","    upd.set_index([[patch_time], [0]], inplace=True)<br>\n","    y_private_df.loc[(patch_time, 0)] = upd"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","if y_private_df.isnull().values.any():<br>\n","    print(\"DataFrame contains NaN values.\")<br>\n","print(y_private_df)"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","assert not y_private_df.isnull().values.any(), \"private contains null\"<br>\n","for col, tot in zip(y_private_df.columns, ntu_tots):<br>\n","    y_private_df[col] = y_private_df[col].clip(lower=0, upper=tot)<br>\n","y_private_test = y_private_df.values<br>\n","private_test_df = pd.DataFrame(<br>\n","    y_private_test, columns=ntu_snos, index=private_test_range<br>\n",")"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","tmp = pd.concat(<br>\n","    [<br>\n","        public_test_df,<br>\n","        private_test_df,<br>\n","    ]<br>\n",")<br>\n","# reset_index: old index => \"time\" column<br>\n","# id_vars: fixed column like index<br>\n","# var_name: columns.name to \"sno\" column<br>\n","# value_name: value => \"sbi\" column<br>\n","tmp = tmp.reset_index(names=\"time\").melt(<br>\n","    id_vars=\"time\", var_name=\"sno\", value_name=\"sbi\"<br>\n",")<br>\n","out_df = pd.DataFrame(<br>\n","    {<br>\n","        \"id\": (<br>\n","            tmp[\"time\"].dt.strftime(\"%Y%m%d\")<br>\n","            + \"_\"<br>\n","            + tmp[\"sno\"]<br>\n","            + \"_\"<br>\n","            + tmp[\"time\"].dt.strftime(\"%H:%M\")<br>\n","        ),<br>\n","        \"sbi\": tmp[\"sbi\"],<br>\n","    }<br>\n",")<br>\n","out_df.to_csv(<br>\n","    f\"../submission/pub_pri_{datetime.now().strftime('%m-%d-%H-%M')}.csv\", index=False<br>\n",")<br>\n","print(\"csv created\")"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","TODO patch private<br><br>\n","Set the initial time"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","<br>\n","<br><br>\n","current_datetime = pd.to_datetime(\"2023-12-10 23:40\")<br><br>\n","current_time = current_datetime.time()<br><br>\n","cur_data = old_tb[old_tb.index == current_datetime]<br><br>\n","print(cur_data)<br><br>\n","cur = old_tb[old_tb.index.to_series().dt.time == current_time]<br><br>\n","# Loop to fetch data for the next 20 minutes<br><br>\n","end_datetime = pd.to_datetime(\"2023-12-04 03:59\")<br><br>\n","next_datetime = current_datetime<br><br>\n","td = timedelta(minutes=20)<br><br>\n","total_td = timedelta(minutes=0)<br><br>\n","while next_datetime <= end_datetime:<br><br>\n","    # Increment current_time by 20 minutes<br><br>\n","    next_datetime += td<br><br>\n","    next_time = next_datetime.time()<br><br>\n","    total_td += td<br><br>\n","    # Filter data from old_tb for the current time<br><br>\n","    nxt = old_tb[old_tb.index.to_series().dt.time == next_time]<br><br>\n","    diff = nxt - cur.shift(freq=total_td)<br><br>\n","    mean_diff = pd.pivot_table(diff.mean().reset_index(), columns=\"sno\")<br><br>\n","    mean_diff.set_index(cur_data.index, inplace=True)<br><br>\n","    upd = cur_data + mean_diff<br><br>\n","    # print(upd)<br><br>\n","    patch_datetime = next_datetime + timedelta(minutes=1)<br><br>\n","    patch_time = patch_datetime.time()<br><br>\n","    upd.set_index([[patch_time], [0]], inplace=True)<br><br>\n","    y_private_df.loc[(patch_time, 0)] = upd<br><br>\n","print(y_private_df)<br><br>\n"]},{"cell_type":"markdown","metadata":{},"source":["%% [markdown]<br>\n","convert the prediction to the required format"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":2}
